set -x\n\nexport PYTHONPATH=$PWD:$PYTHONPATH\n\n# Select the model type\nexport MODEL_TYPE=\"FLUX-AFCache\"\n# Configuration for different model types\n# script, model_id, inference_step\ndeclare -A MODEL_CONFIGS=(\n    [\"FLUX-AFCache\"]=\"./afcache_flux_pipeline.py /home/lyb/FLUX.1-dev 50\"\n)\n\nif [[ -v MODEL_CONFIGS[$MODEL_TYPE] ]]; then\n    IFS=' ' read -r SCRIPT MODEL_ID INFERENCE_STEP <<< \"${MODEL_CONFIGS[$MODEL_TYPE]}\"\n    export SCRIPT MODEL_ID INFERENCE_STEP\nelse\n    echo \"Invalid MODEL_TYPE: $MODEL_TYPE\"\n    exit 1\nfi\n\nmkdir -p ./results\n\n# task args\nTASK_ARGS=\"--height 1024 --width 1024 --no_use_resolution_binning\"\n\n# pipefusion is not recommended for cache based methods.\n# because the computation is greatly reduced,\n# and the communication cost cannot be hidden asynchronously.\n\nN_GPUS=4\nPARALLEL_ARGS=\"--pipefusion_parallel_degree 1 --ulysses_degree 2 --ring_degree 2\"\n\nmkdir -p ./results\n\ntorchrun --nproc_per_node=$N_GPUS $SCRIPT \\\n--model $MODEL_ID \\\n$PARALLEL_ARGS \\\n$TASK_ARGS \\\n--num_inference_steps $INFERENCE_STEP \\\n--warmup_steps 1 \\\n--prompt \"A cat standing on the grass.\"\n